# Evals

Evals determines if the LLM output is bad.

Evals can look at your user_query, prompt, response, and any ground truth data (ex: retrieved context) to determine if the output “passes” or “fails”

We have a number of preset evals, that you can configure to your use case.

Alternately, you can write your own evals using our [custom eval template](logging/langchain.mdx).
