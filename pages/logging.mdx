import { Callout } from "nextra/components";

## Logging

To get started with Athina's Monitoring, the first step is to start logging your inferences.

<Callout>
Logging your inferences is very simple - just a single `POST` request to Athina's logging endpoint.

You should start with these instructions: [Logging via API Request](/logging/log_via_api).

</Callout>

If you are using **OpenAI with streaming**, then follow these instructions:

- [OpenAI Chat Completion (1.x)](/logging/openai_chat_1)
- [OpenAI Chat Completion (0.x)](/logging/openai_chat_0)
- [OpenAI Completion (1.x)](/logging/openai_completion_1)
- [OpenAI Completion (0.x)](/logging/openai_completion_0)

If you are using **Langchain**, then follow these instructions:

- [Langchain](/logging/langchain)

**For all other models, follow these instructions:**

- [Anthropic](/logging/anthropic)
- [Meta Llama](/logging/meta_llama)
- [All Other Models](/logging/misc)

---

### FAQs

- [Will Athina logging add additional latency](/logging/faq/logging_latency)
- [Do I have to use Athina as a proxy for logging](/logging/faq/proxy)
- [Where is my data stored](/logging/faq/data_policy)

---

### Tracing

Recent LLM applications have many intricate abstractions (chains, tool-equipped agents, sophisticated prompts). Athina's nested tracing clarifies operations and pinpoints problem origins. The Athina SDK and UI facilitate tracing advanced LLM features, such as vector database queries and multiple LLM interactions, by allowing easy chaining of them in the SDK.

Head over to [Tracing](/logging/tracing) to learn more.