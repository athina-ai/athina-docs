# Prebuilt Evals

Evals determines if the LLM output is bad.

Evals can look at your user_query, prompt, response, and any ground truth data (ex: retrieved context) to determine if the output “passes” or “fails”

We have a number of general purpose evals (described below), but you can also configure these evals, or even write your own eval.
