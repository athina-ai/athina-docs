## Preset Evals

You can use our preset evaluators to add evaluation to your dev stack rapidly.

Here are our preset evaluators:

#### RAG Evals

[These evals](/evals/preset_evals/rag_evals) are useful for evaluating LLM applications with Retrieval Augmented Generation (RAG).

- [Context Contains Enough Information](/evals/preset_evals/ccei)
- [Context Relevance](/evals/preset_evals/context_relevance)
- [Answer Relevance](/evals/preset_evals/answer_relevance)
- [Does Response Answer Query](/evals/preset_evals/draq)
- [Response Faithfulness](/evals/preset_evals/faithfulness)

<br />

#### Summarization Evals:

These evals are useful for evaluating LLM-powered summarization performance.

- [Summarization Hallucination](/evals/preset_evals/summarization_eval)
- [Summarization Informativeness](/evals/preset_evals/summarization_eval)

<br />

#### Grounded Evals:

These evals also require an "ideal response" to compare against.

- [Answer Similarity](/evals/preset_evals/answer_similarity)

<br />

#### Miscellaneous Evals

- [Response Verbosity](/evals/preset_evals/verbosity) (Coming soon)
- [Language Mismatch](/evals/preset_evals/language_mismatch) (Coming soon)
- [Response contains PII](/evals/preset_evals/pii) (Coming soon)
- [Response contains toxic content](/evals/preset_evals/toxicity) (Coming soon)

---

#### Function Evals

These evals do not use an eval model - they are just simple functions.

- Response contains a link
- Response contains an invalid link
- Response contains keywords
- Response is missing keywords
