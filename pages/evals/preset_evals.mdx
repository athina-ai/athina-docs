## Preset Evals

You can use our preset evaluators to add evaluations to your dev stack rapidly.

Here are our preset evaluators:

#### RAG Evals

[These evals](/evals/preset_evals/rag_evals) are useful for evaluating LLM applications with Retrieval Augmented Generation (RAG).

- [Context Contains Enough Information](/evals/preset_evals/ccei)
- [Does Response Answer Query](/evals/preset_evals/draq)
- [Response Faithfulness](/evals/preset_evals/faithfulness)
- [Groundedness](/evals/preset_evals/groundedness)

<br />

#### RAGAS Evals

[RAGAS](/evals/preset_evals/ragas_evals) is a popular library with state-of-the-art evaluation metrics for RAG models:

- [Context Precision](/evals/preset_evals/ragas_evals#context-precision)
- [Context Relevancy](/evals/preset_evals/ragas_evals#context-relevancy)
- [Context Recall](/evals/preset_evals/ragas_evals#context-recall)
- [Faithfulness](/evals/preset_evals/ragas_evals#faithfulness)
- [Answer Relevancy](/evals/preset_evals/ragas_evals#answer-relevancy)
- [Answer Semantic Similarity](/evals/preset_evals/ragas_evals#answer-semantic-similarity)
- [Answer Correctness](/evals/preset_evals/ragas_evals#answer-correctness)
- [Coherence](/evals/preset_evals/ragas_evals#coherence)
- [Conciseness](/evals/preset_evals/ragas_evals#conciseness)
- [Maliciousness](/evals/preset_evals/ragas_evals#maliciousness)
- [Harmfulness](/evals/preset_evals/ragas_evals#harmfulness)

---

#### Summarization Evals:

These evals are useful for evaluating LLM-powered summarization performance.

- [Summarization Accuracy](/evals/preset_evals/summarization_eval)

---


#### Custom Evals

These evals can help you create custom evaluation conditions.

- [Grading Criteria](/evals/custom_evals#custom-grading-criteria-easy)
- [Custom Prompt](/evals/custom_evals#llm-evaluator-with-a-completely-custom-prompt)

---

#### Function Evals

Unlike the previous evaluators which used an LLM for grading, function evals do not use an LLM - they just use simple functions. For example the evaluator checks if the

- Response matches a given [regular expression](/evals/preset_evals/function_evals#regex)
- Response [contains a link](/evals/preset_evals/function_evals#containslink)
- Response [contains keywords](/evals/preset_evals/function_evals#contains-any)
- Response [contains no invalid links](/evals/preset_evals/function_evals#noinvalidlinks)
- Response is [missing keywords](/evals/preset_evals/function_evals#containsall)
- and more

Head over to the [function evaluators](/evals/preset_evals/function_evals) page for further details.

---

#### Grounded Evals

So far all the previous evaluators are ungrounded - meaning that they do not compare the response against any reference data. Following are evaluators that can compare the LLM generated response against the expected_response or context. For example
- [Answer similarity](/evals/preset_evals/grounded_evals#answer_similarity)
- [Context similarity](/evals/preset_evals/grounded_evals#context_similarity)

Head over to the [grounded evaluators](/evals/preset_evals/grounded_evals) page for further details.
