## Preset Evals

You can use our preset evaluators to add evaluations to your dev stack rapidly.

Here are our preset evaluators:

#### RAG Evals

[These evals](/evals/preset_evals/rag_evals) are useful for evaluating LLM applications with Retrieval Augmented Generation (RAG).

- [Context Contains Enough Information](/evals/preset_evals/ccei)
- [Does Response Answer Query](/evals/preset_evals/draq)
- [Response Faithfulness](/evals/preset_evals/faithfulness)

<br />

#### Summarization Evals:

These evals are useful for evaluating LLM-powered summarization performance.

- [Summarization Accuracy](/evals/preset_evals/summarization_eval)

<br />

#### Custom Evals

These evals can help you create custom evaluation conditions.

- [Grading Criteria](/evals/custom_evals#custom-grading-criteria-easy)
- [Custom Prompt](/evals/custom_evals#llm-evaluator-with-a-completely-custom-prompt)

<br />

---

#### Function Evals

Unlike the previous evaluators which used an LLM for grading, function evals do not use an LLM - they just use simple functions. For example the evaluator checks if the 

- Response matches a given [regular expression](/evals/preset_evals/function_evals#regex)
- Response contains a link
- Response contains an invalid link
- Response [contains keywords](/evals/preset_evals/function_evals#contains-any)
- Response is missing keywords
- so on and so forth

Head over to the [function evaluators](/evals/preset_evals/function_evals) for further details.