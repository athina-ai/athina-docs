# Preset Evals

You can use our preset evaluators to add evaluations to your dev stack rapidly.

Here are our preset evaluators:

#### RAG Evals

[These evals](/evals/preset_evals/rag_evals) are useful for evaluating LLM applications with Retrieval Augmented Generation (RAG).

- [Context Contains Enough Information](/evals/preset_evals/ccei)
- [Does Response Answer Query](/evals/preset_evals/draq)
- [Response Faithfulness](/evals/preset_evals/faithfulness)
- [Groundedness](/evals/preset_evals/groundedness)

<br />

#### RAGAS Evals

[RAGAS](/evals/preset_evals/ragas_evals) is a popular library with state-of-the-art evaluation metrics for RAG models:

- [Context Precision](/evals/preset_evals/ragas_evals#context-precision)
- [Context Relevancy](/evals/preset_evals/ragas_evals#context-relevancy)
- [Context Recall](/evals/preset_evals/ragas_evals#context-recall)
- [Faithfulness](/evals/preset_evals/ragas_evals#faithfulness)
- [Answer Relevancy](/evals/preset_evals/ragas_evals#answer-relevancy)
- [Answer Semantic Similarity](/evals/preset_evals/ragas_evals#answer-semantic-similarity)
- [Answer Correctness](/evals/preset_evals/ragas_evals#answer-correctness)
- [Coherence](/evals/preset_evals/ragas_evals#coherence)
- [Conciseness](/evals/preset_evals/ragas_evals#conciseness)
- [Maliciousness](/evals/preset_evals/ragas_evals#maliciousness)
- [Harmfulness](/evals/preset_evals/ragas_evals#harmfulness)

---

#### Safety Evals

These evals are useful for evaluating LLM applications with safety in mind.

- [PII Detection](/evals/preset_evals/safety/pii_detection): Will fail if PII is found in the text
- [Prompt Injection](/evals/preset_evals/safety/prompt_injection): Will fail if any known Prompt Injection attack is found the text. Learn more about [Prompt Injection](/guides/prompt_injection).
- [Maliciousness](/evals/preset_evals/ragas_evals#maliciousness): Measures maliciousness of the response
- [Harmfulness](/evals/preset_evals/ragas_evals#harmfulness): Measures harmfulness of the response

---

#### Summarization Evals:

These evals are useful for evaluating LLM-powered summarization performance.

- [Summarization Accuracy](/evals/preset_evals/summarization_eval)

---

#### Custom Evals

These evals can help you create custom evaluation conditions.

- [Grading Criteria](/evals/custom_evals#custom-grading-criteria-easy)
- [Custom Prompt](/evals/custom_evals#llm-evaluator-with-a-completely-custom-prompt)

---

#### Function Evals

Unlike the previous evaluators which used an LLM for grading, function evals do not use an LLM - they just use simple functions. For example the evaluator checks if the

- `text` matches a given [regular expression](/evals/preset_evals/function_evals#regex)
- `text` [contains a link](/evals/preset_evals/function_evals#containslink)
- `text` [contains keywords](/evals/preset_evals/function_evals#contains-any)
- `text` [contains no invalid links](/evals/preset_evals/function_evals#noinvalidlinks)
- `text` is [missing keywords](/evals/preset_evals/function_evals#containsall)
- and more

Head over to the [function evaluators](/evals/preset_evals/function_evals) page for further details.

---

#### Evals with ground-truth

So far all the previous evaluators do not compare the response against any reference data. Following are evaluators that can compare the LLM generated response against the `expected_response` or `context`. For example

- [Answer Similarity](/evals/preset_evals/grounded_evals#answer_similarity)
- [Context Similarity](/evals/preset_evals/grounded_evals#context_similarity)

Head over to the [grounded evaluators](/evals/preset_evals/grounded_evals) page for further details.
