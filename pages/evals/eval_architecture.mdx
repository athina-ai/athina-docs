# Eval Architecture

<img src="/architecture.png" />

We provide LLM evaluators  for RAG chatbots, text summarization, and LLM Agents. Additionally, we evaluate specialized tasks such as sentiment detection and PII detection within any LLM output. Furthermore, we can incorporate any custom evaluation metric tailored to your use-case.

We run the evaluations on your logged data. Using a cron scheduler, you have the flexibility to determine when evaluations are run â€“ even real-time. Evaluations can be run multiple times to ensure consistent results.

Each of our evaluators is paired with a specific metric. This metric could be a numerical score or a binary output, depending on the case at hand. If labeled data is available, traditional machine learning metrics such as accuracy, recall, and precision can be applied.

Any evaluation yielding a score above a set threshold is flagged as a failure by default. However, we offer the flexibility to set your own criteria for failure, even if it means combining multiple scores.