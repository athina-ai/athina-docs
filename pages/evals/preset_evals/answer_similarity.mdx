import { Callout } from "nextra/components";

### Answer Similarity

_Coming soon_

### Info

Checks the similarity between the LLM-generated response and an ideal response from your training dataset.

A great way to measure your model's performance is to compare the ideal response (`expected_response`) against the LLM-generated `response`.

**Required Args**

- `expected_response`: The ideal response you would like your LLM to provide.
- `response`: The LLM generated response.

**Optional Args**

- `strategy`: `cosine_similarity` | `edit_distance`

Default `strategy`: cosine_similarity

**Metrics**

- `similarity_score`: float
  - `minValue`: 0 - represents a completely dissimilar answer
  - `maxValue`: 1 - represents an exact match between ideal response and the LLM-generated response.

---

### Example

<Callout type='infos'>
- **Response:** YC invests $500,000 in 200 startups and takes around 7% equity from each.
- **Expected Response:** YC invests $500,000 in exchange for 7% equity.

</Callout>

<Callout type='info'>
**Eval Result**
- **similarity_score:** 0.85
- **Explanation:** The response mentions that YC takes 5-7% equity, but this is not mentioned anywhere in the context.

</Callout>

---

### Run the eval on a dataset

1. Load your data with the `RagLoader`

```python
# Load the data from CSV, JSON, Athina or Dictionary
dataset = RagLoader().load_json(json_file)
```

2. Run the evaluator on your dataset

```python
Faithfulness().run_batch(data=dataset)
```

### Run the eval on a single datapoint

```python
Faithfulness().run(
    context=context,
    response=response
)
```
