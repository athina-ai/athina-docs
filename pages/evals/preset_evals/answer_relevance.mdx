import { Callout } from "nextra/components";

### Answer Relevance

_Coming soon_

_This is an [LLM Graded Evaluator](/evals/faq/why_does_llm_eval_work)_

### Info

Checks if the LLM-generated `response` is relevant to the `query`.

This evaluator will return a relevance score to indicate how relevant the response is to the provided query.

**Required Args**

- `query`: The query
- `response`: The LLM generated response

**Metrics**

- `relevance_score`: float
  - `minValue`: 0 - represents a completely irrelevant answer
  - `maxValue`: 1 - represents a perfectly relevant answer

**Default Engine:** `gpt-4`

---

### Low Relevance Example

<Callout type='infos'>
- **Query:** Who is the president of the United States?
- **Response:** The capital of the United States is Washington, DC.

</Callout>

<Callout type='error'>
**Eval Result**
- **relevance_score:** 0.32 (low relevance)
- **Explanation:** The response mentions that YC takes 5-7% equity, but this is not mentioned anywhere in the context.

</Callout>

### High Relevance Example

<Callout type='infos'>
- **Query:** Who was the first person to land on the moon?
- **Response:** Neil Armstrong, an American astronaut, first set foot on the moon in 1969 on the Apollo 11 mission.

</Callout>

<Callout type='info' emoji='âœ…'>
**Eval Result**
- **relevance_score:** 0.91 (high relevance)
- **Explanation:** The response mentions that YC takes 5-7% equity, but this is not mentioned anywhere in the context.

</Callout>
