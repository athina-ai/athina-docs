import { Callout } from "nextra/components";

## Custom Evals

_How can you write your own evals?_

### Custom Grading Criteria (Easy)

It's very easy to write a custom grading criteria (just 2 lines of code).

```python
from athina.evals import GradingCriteria

grading_criteria="If the response says to contact customer support, then fail. Otherwise pass."
GradingCriteria(grading_criteria=grading_criteria).run_batch(data=dataset)
```

[See an example notebook â†’](https://github.com/athina-ai/athina-evals).

<Callout type="default" emoji="ðŸ’¡">
  Note: This format works pretty well for the grading_criteria: _"If X, then
  fail. Otherwise, pass"_
</Callout>

<Callout type="info">
**What's happening under the hood?**

We do a few things behind the scenes to make LLM evaluators work effectively:

- We wrap this prompt inside some [chain-of-thought prompting](https://www.promptingguide.ai/techniques/cot)
- We ensure the response format is always JSON, and includes a Pass/Fail `result` and `explanation`

</Callout>

---

### LLM Evaluator with a Completely Custom Prompt

If you have a more complex evaluation prompt that you would like to run within Athina's framework, we can support that with our `CustomPrompt` class.

- Simply use the `CustomPrompt` class and specify your own `eval_prompt`.

```python
eval_prompt = """
Think step-by-step.

Based on the provided user query and refund policy, determine if the response adheres to the refund policy.

User Query: {query}
Refund policy: {context}
Response: {response}
"""

batch_run_result = CustomPrompt(
    display_name="Response must follow refund policy",
    required_args=["query", "context", "response"],
    model="gpt-4-1106-preview",
    eval_prompt=eval_prompt,
).run_batch(data=dataset)
```

[See an example notebook â†’](https://github.com/athina-ai/athina-evals).

<Callout type="default" emoji="ðŸ’¡">
  Note: Any variables you use in the prompt (for example: `query`, `context`,
  `response`) will be interpolated from your `dataset`.
</Callout>

---

### Write your own LLM Eval class

You can extend the `LlmEvaluator` class. See this [example](https://github.com/athina-ai/athina-evals/blob/main/athina/evals/llm/faithfulness/evaluator.py)

This will allow you to:

- Override the system and user prompts.
- Add custom few-shot examples to improve performance for your use case.
- Add custom metrics.
- Pass custom arguments into your LLM Evaluator.
