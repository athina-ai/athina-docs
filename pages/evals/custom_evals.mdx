import { Callout } from "nextra/components";

## Custom Evals

_How can you write your own evals?_

### Custom Grading Criteria (Easy)

It's very easy to write a custom grading criteria (just 2 lines of code).

```python
from athina.evals import CustomGrader

# Custom evaluator
# Checks if the response mentions black holes
grading_criteria="If the response mentions black holes, then fail. Otherwise pass."
CustomGrader(grading_criteria=grading_criteria).run_batch(data=dataset)
```

<Callout type="default" emoji="ðŸ’¡">
  Note: This format works pretty well for the grading_criteria: _"If X, then
  fail. Otherwise, pass"_
</Callout>

<Callout type="info">
**What's happening under the hood?**

We do a few things behind the scenes to make LLM evaluators work effectively:

- We wrap this prompt inside some [chain-of-thought prompting](https://www.promptingguide.ai/techniques/cot)
- We ensure the response format is always JSON, and includes a Pass/Fail `result` and `explanation`

</Callout>

---

### LLM Evaluator with a Custom Prompt

You can also quickly create a new eval with your own custom prompt.

- define a `system_message_template` (string), and a `user_message_template` (string).

- initialize the `CustomLlmEval` class.

```python
# Checks if the LLM response answers the user query sufficiently
display_name = "FaithfulnessV2"
required_args = ["query", "response"]
batch_run_result = CustomLlmEval(
    display_name=display_name,
    metric_id=None,
    required_args=required_args,
    model="gpt-4-1106-preview",
    system_message_template=system_message_template,
    user_message_template=user_message_template,
).configure_experiment(experiment=experiment).run_batch(
    data=dataset,
    max_parallel_evals=10
)
```

---

### Write your own LLM Eval class

You can extend the `LlmEvaluator` class. See this [example](https://github.com/athina-ai/athina-evals/blob/main/athina/evals/llm/faithfulness/evaluator.py)

This will allow you to:

- Override the system and user prompts.
- Add custom few-shot examples to improve performance for your use case.
- Add custom metrics.
- Pass custom arguments into your LLM Evaluator.

### Can I specify a different model?

Yes, you can specify a different model for running evals.

Currently, we support the following OpenAI models:

- `gpt-4-1106-preview`
- `gpt-4`
- `gpt-3.5-turbo`

```python
from athina.evals import LlmEvaluator

LlmEvaluator(model="gpt-4", grading_criteria=grading_criteria).run(response)
```

If you'd like to use a different model, contact us at hello@athina.ai.

### Can I pass parameters other than the response?

Yes, you can do this by extending the `LlmEvaluator` class.

_We will also support this in our UI soon_
