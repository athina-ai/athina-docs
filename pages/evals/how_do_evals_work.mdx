### How do evals work?

Most evaluators use another LLM.

Some evaluators will not need another LLM - for example, we can figure out if there is a bad link in your output simply by using regex + an HTTP request.

Similarly, for detecting toxic or violent content, we can simple make a request to OpenAI’s content moderation endpoint

However, many of the most useful evals will use another LLM.

This is because we are dealing with complex, unstructured textual data. And there’s no better tool at handling that than another LLM.
