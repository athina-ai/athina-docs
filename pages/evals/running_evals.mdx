import { Callout } from "nextra/components";

# Running Evals

There are a few ways to run evals using Athina:

- [Run evals using Python SDK](/evals/running_evals/run_eval_suite)
- [Run evals on a dataset using Athina Platform](/evals/running_evals/run_on_athina_platform)
- Compare 2 datasets side by side with evaluation metrics
- [Run evals as real-time guardrails using `athina.guard()`](/guides/athina_guard)
- [Configure evals to run continuously on Production Traces](/evals/automatic_evals)

### Running evals programmatically using Python SDK

<Callout>

Here's a 2-minute [video tutorial](https://www.loom.com/share/10e37f1ba11242ac8c97902edd2fa61e)
showcasing how you can quickly run pre-built evals, and view the results on the dashboard.

</Callout>

The easiest way to get started is to use one of our [Example Notebooks](/evals/cookbooks) as a starting point.

For more detailed guides, you can follow the links below to get started running evals using Athina.

- [Quick Start Guide](/evals/quick_start)
- [Run an eval](/evals/running_evals/run_eval)
- [Run an eval suite](/evals/running_evals/run_eval_suite)
- [Customize an eval](/evals/custom_evals)
- [View Results on Athina Dashboard](/evals/develop_dashboard)
- [Loading Data for Evals](/evals/loading_data)

---

### Configure evals to run continuously on Production Traces

If you configure evaluations in the dashboard at https://app.athina.ai/evals/config, they will run automatically against all logged inferences that meet your filters.

- [How to configure automatic evals](/evals/automatic_evals)

**Note:** Logs may be sampled to ensure that evaluations run within your configured limits. You can adjust these limits in the [Settings](https://app.athina.ai/settings) page.

**Note: Continuous evaluation is only available for paid plans. Contact hello@athina.ai to upgrade your plan.**

---

### Running evals as guardrails around inference using `athina.guard()`

This is useful if you want to run evaluations at inference time to prevent bad user queries or bad responses.

Keep in mind there may be latency impacts here. We recommend running only [low-latency evaluations](/evals/low_latency_evals) if you're using `athina.guard()`.

Follow this [example notebook](https://github.com/athina-ai/athina-evals/blob/main/examples/guard.ipynb)

---

### Run a single eval manually from the Inference (Trace) page.

1. Open the inference you want to evaluate, and click the "Run Eval" button (located towards the top-right).
2. Choose the evaluation you want to run (Note: function evals cannot be run from the inference page).
3. Choose the LLM engine for your evaluation.

Eval results will appear shortly in the Evals tab on the right.

<br />

<img
  src="/run-eval-manually.gif"
  alt="Run an eval manually from the inference page"
/>

---
