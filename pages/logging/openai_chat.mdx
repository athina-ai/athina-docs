import { Callout } from "nextra/components";

# OpenAI Chat Completions

<Callout emoji="!!">
  If you are using python, we would highly recommend that you use [this logging
  method](./openai_chat_python) instead.
</Callout>

##### Supported Models

- `gpt-3.5-turbo`
- `gpt-3.5-turbo-0613`
- `gpt-3.5-turbo-16k`
- `gpt-3.5-turbo-16k-0613`
- `gpt-4`
- `gpt-4-0613`
- `gpt-4-32k`
- `gpt-4-32k-0613`

---

### Log Via Python SDK

##### Install the Python SDK

Run `pip install athina-logger`

##### Set Athina API key

```python
# Initialize the Athina API key somewhere in your code
AthinaApiKey.set_api_key(os.getenv('ATHINA_API_KEY'))
```

##### Log Inference Calls

```python
import openai
from athina_logger.inference_logger import InferenceLogger
from athina_logger.api_key import AthinaApiKey
from athina_logger.exception.custom_exception import CustomException

def log_llm_response():
	# Prompt Messages Array
	messages = [
	    {
			"role": "system",
			"content": ...
	    },
		...
	]

	# Retrieved data from your RAG system
	context = {
		"information": retrieved_context
	}


	# Your LLM inference call + some code to get response times
	start_time = time.time()
	chat_completion = openai.ChatCompletion.create(model=model, messages=messages)
	end_time = time.time()
	response_time_ms = int((end_time - start_time) * 1000)


	# Log data to Athina Server
	# ---
	try:
		InferenceLogger.log_open_ai_chat_response(
			prompt_slug="customer_support", # REQUIRED
			user_query=user_query, # OPTIONAL, highly recommended for Q&A apps
			messages=messages, # REQUIRED
			model=model, # REQUIRED
			completion=chat_completion, # OPTIONAL
			context=context, # OPTIONAL
			response_time=response_time_ms, # OPTIONAL
			customer_id="stripe", # OPTIONAL - Your customer's internal ID
			customer_user_id="shiv@athina.ai", # OPTIONAL - Your end user's internal ID
			session_id="c45g-1234-s6g4-43d3", # OPTIONAL - Your internal session / conversation ID
			environment="production", # OPTIONAL
			external_reference_id="5e838eaf-7dd0-4b6f-a32c-26110dd54e58", # OPTIONAL, your internal unique identifier to track the inference call
		)
	 except Exception as e:
        if isinstance(e, CustomException):
            print(e.status_code)
            print(e.message)
        else:
            print(e)
...
```

- If you do not have access to the `completion` response object from OpenAI, then pass the following fields instead:
  - `prompt_response`: generated response from OpenAI as a raw string
  - `prompt_tokens`: integer value representing the token count of your prompt
  - `completion_tokens`: integer value representing the token count of your completion response
  - `total_tokens`: prompt_tokens + completion_tokens

---

### Log via API Request

**Method**: `POST`

**Endpoint**: `https://api.athina.ai/api/v1/log/prompt/openai-chat`

**Headers**: `athina-api-key`: YOUR_ATHINA_API_KEY

**Request Body**

```json
{
    // Prompt identifier
    prompt_slug: "customer_support",
    // Messages array sent to OpenAI
    prompt_messages: messages,
    // Model ID used for inference (ex: gpt-3.5-turbo)
    language_model_id: language_model_id,
	// functions parameter for function call feature in chat completion
	functions: functions
    // OpenAI chat completion object (OPTIONAL)
    completion: chatCompletion,
	// OpenAI chat completion prompt response text (OPTIONAL)
	prompt_response: "this is openai chat completion response in text"
    // your ground truth data object (OPTIONAL)
    context: context,
    // response time in milliseconds (OPTIONAL)
    response_time: responseTimeMs,
    // your customer ID (business) (OPTIONAL)
    customer_id: "stripe",
    // your customer user ID (business user) (OPTIONAL)
    customer_user_id: "shiv@athina.ai",
    // your session or conversation ID (OPTIONAL)
    session_id: "c45g-1234-s6g4-43d3",
    // the user's query (for chat applications) (OPTIONAL)
    user_query: user_query,
    // "production" or "development" (OPTIONAL)
    environment: "production",
	// unique external identifier; should be unique across all inference calls
	external_reference_id: "5e838eaf-7dd0-4b6f-a32c-26110dd54e58",
	// tokens used in the prompt (OPTIONAL)
	prompt_tokens: 10,
	// tokens used in the completion response (OPTIONAL)
	completion_tokens: 20,
	// total tokens -> prompt_tokens + completion_tokens (OPTIONAL)
	total_tokens: 30

}
```

**Sample functions field in the request body (only available in the HTTP request; not in python SDK for now)**

```json
functions: [
    {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"]
          }
        },
        "required": ["location"]
      }
    }
  ]
```

## Not using Python?

Reach out to us at hello@athina.ai - we're happy to add support for other stacks as well if we hear from you.
