import { Tabs } from "nextra/components";

# OpenAI Chat Completion

_If you're using OpenAI chat completions in Python, you can get set up in just **2 minutes**_

<Tabs items={[ "API Request (Recommended)", "Python Wrapper (Easy)", "Python (Streaming)", "Python (Non-Streaming)"]}>
  <Tabs.Tab>
    ### Log via API Request

    See instructions [here](/logging/log_via_api).

  </Tabs.Tab>
  <Tabs.Tab>
    ##### 1. Install the Python SDK

    Run `pip install athina-logger`

    ##### 2. Import Athina Logger

    Replace your `import openai` with this:

    ```python
    from athina_logger.api_key import AthinaApiKey
    from athina_logger.athina_meta import AthinaMeta
    from athina_logger.openai_wrapper import openai

    openai.api_key = os.getenv('OPENAI_API_KEY')
    ```

    ##### 3. Set Athina API key

    ```python
    # Initialize the Athina API key somewhere in your code
    AthinaApiKey.set_api_key(os.getenv('ATHINA_API_KEY'))
    ```

    ##### 4. Use OpenAI ChatCompletion request as you do normally

    ```python
    # Use openai.ChatCompletion just as you would normally
    # Add fields to AthinaMeta for better segmentation of your data
    openai.ChatCompletion.create(
        model="gpt-4",
        messages=messages,
        stream=False,
        athina_meta=AthinaMeta(
            prompt_slug="yc_rag_v1",
            user_query="How much funding does Y Combinator provide?", # For RAG Q&A systems, log the user's query
            context={"information": retrieved_documents} # Your retrieved documents
            session_id=session_id, # Conversation ID
            customer_id=customer_id, # Your Customer's ID
            customer_user_id=customer_id, # Your End User's ID
            environment=environment, # Environment (production, staging, dev, etc)
            external_reference_id="ext_ref_123456",
        ),
    )
    ```

    ---

    ### Frequently Asked Questions

    ##### Q. What is AthinaMeta

    _The `AthinaMeta` fields are used for segmentation of your data on the dashboard. All these fields are optional, but highly recommended._

    ```python
    class AthinaMeta:
        prompt_slug: Optional[str] = None
        context: Optional[dict] = None
        customer_id: Optional[dict] = None
        customer_user_id: Optional[dict] = None
        session_id: Optional[dict] = None
        user_query: Optional[dict] = None
        environment: Optional[dict] = None
        external_reference_id: Optional[dict] = None
        customer_id: Optional[str] = None
        customer_user_id: Optional[str] = None
        response_time: Optional[int] = None
    ```


    ##### Q. Is this SDK going to make a proxy request to OpenAI through Athina?

    Nope! We know how important your OpenAI inference call is, so we don't want to interfere with that or increase response times.

    Instead, we simply make an logging API request to Athina, which is separate from your OpenAI request.

    ##### Q. Will this SDK increase my latency?

    Nope! The logging call is being made in a background thread as a fire and forget request, so there is almost no additional latency (< 5ms).

  </Tabs.Tab>
  
  <Tabs.Tab>
    ##### 1. Install the Python SDK

    Run `pip install athina-logger`

     ##### 2. Import Athina Logger

    ```python
    from athina_logger.api_key import AthinaApiKey
    import openai

    openai.api_key = os.getenv('OPENAI_API_KEY')
    ```

    ##### 3. Set Athina API key

    ```python
    # Initialize the Athina API key somewhere in your code
    AthinaApiKey.set_api_key(os.getenv('ATHINA_API_KEY'))
    ```

    ##### 4. Call the logging function

    ```python

    # Using python openai SDK
    def stream_openai_chat_response(user_query, content):
        response = openai.ChatCompletion.create(
            model='gpt-3.5-turbo',
            messages=[{"role": "user", "content": content}],
            stream=True,
        )
        logger = LogOpenAiChatCompletionStreamInference(
            prompt=[{"role": "user", "content": content}],
            language_model_id="gpt-3.5-turbo",
            prompt_slug="test",
            context=context, # OPTIONAL
            response_time=response_time_ms, # OPTIONAL
            customer_id="stripe", # OPTIONAL
            customer_user_id="shiv@athina.ai", # OPTIONAL
            session_id="c45g-1234-s6g4-43d3", # OPTIONAL
            user_query=user_query, # OPTIONAL
            environment="production", # OPTIONAL
            external_reference_id="5e838eaf-7dd0-4b6f-a32c-26110dd54e58", # OPTIONAL; If passed, should be unique across all inference calls
        )
    ```

    ##### 5. Collect the streaming responses
    There are 2 ways to collect openai chat streams

    **Option 1: Collect automatically from response**
    ```python
    try:
        logger.collect_stream_inference(response)
        logger.log_stream_inference()
    except Exception as e:
        if isinstance(e, CustomException):
            print(e.status_code)
            print(e.message)
        else:
            print(e)
    ```

    **Option 2: Collect individually by chunk**
    ```python
    try:
        for chunk in response:
            logger.collect_stream_inference_by_chunk(chunk)
        logger.log_stream_inference()
    except Exception as e:
        if isinstance(e, CustomException):
            print(e.status_code)
            print(e.message)
        else:
            print(e)
    ```

  </Tabs.Tab>

  <Tabs.Tab>
    ### Log via Python SDK

    See instructions [here](/logging/log_via_python_sdk).

  </Tabs.Tab>

</Tabs>
