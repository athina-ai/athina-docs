import { Callout } from "nextra/components";

<Callout emoji="!!">
  Make sure you have already completed the [Installation &
  Setup](../../quickstart) steps before logging your data.
</Callout>

# Other Language models

### Supported Models

- any non-OpenAI and non-meta-llama model

### Log via Python SDK

```python

from athina_logger.inference_logger import InferenceLogger
from athina_logger.api_key import AthinaApiKey
from athina_logger.exception.custom_exception import CustomException

AthinaApiKey.set_api_key('ATHINA_API_KEY')

def log_llm_response():
    user_query = "What is the pricing structure of Stripe's payment gateway?"
    prompt = f"You are an AI... Human: {user_query}"


    # Ground truth data - log as much structured data as possible
    #
    # This data can be referenced in your test cases.
    # The more structured data you can pass here, the more granular the tests we can write.
    context = {
        "user_query": user_query,
        "user_id": "123-abc-456-xyz",
        "product_name": "Stripe",
        "product_description": {...},
        "pricing": {...},
        "help_data": {...},
        "language_model_id": "MODEL_NAME" # add model name here in the context
    }


    # Your inference call
    llm_response = ... # Response output string from the LLM


    # Extract these values from the LLM response
    prompt_tokens = ...   # int
    completion_tokens = ...   # int
    total_tokens = ...   # int
    response_time = ...  # int representing milliseconds

    # Log data to Athina Server
    try:
        InferenceLogger.log_generic_response(
            prompt=prompt,
            model="generic",
            prompt_response=llm_response,
            prompt_slug="customer_support",
            prompt_tokens=10, # OPTIONAL
            completion_tokens=20, # OPTIONAL
            total_tokens=30, # OPTIONAL
            response_time=response_time, # OPTIONAL
            context=context, # OPTIONAL
            customer_id="stripe", # identifier for your customer # OPTIONAL
            customer_user_id="vitalik@ethereum.org", # identifier for the user interacting with the chatbot # OPTIONAL
            user_query=user_query,	# the user's query message to the chatbot # OPTIONAL
            session_id="c45g-1234-s6g4-43d3" # the ID you use to keep track of the conversation / session # OPTIONAL
            environment="production", # OPTIONAL; default value: production
            external_reference_id="5e838eaf-7dd0-4b6f-a32c-26110dd54e58" # OPTIONAL; unique across all inference calls
        )
    except Exception as e:
        if isinstance(e, CustomException):
            print(e.status_code)
            print(e.message)
        else:
            print(e)
```

### Log via API Request

**Method**: `POST`

**Endpoint**: `https://log.athina.ai/api/v1/log/prompt/generic`

**Headers**: `athina-api-key`: YOUR_ATHINA_API_KEY

**Request Body**

```json
{
    // Prompt identifier (OPTIONAL); default value: "default"
    prompt_slug: "customer_support",
    // Prompt string
    prompt_text: "What are some good restaurants in London?",
    // Model ID used for inference
    language_model_id: "generic",
    // LLM Output
    prompt_response: "Here are some popular restaurants in London...",
    // your ground truth data object (OPTIONAL)
    context: context,
    // prompt tokens (OPTIONAL)
    prompt_tokens: 20
    // completion tokens (OPTIONAL)
    completion_tokens: 30
    // total tokens (OPTIONAL)
    total_tokens: 50
    // response time in milliseconds (OPTIONAL)
    response_time: responseTimeMs,
    // your customer ID (business) (OPTIONAL)
    customer_id: "stripe",
    // your customer user ID (business user) (OPTIONAL)
    customer_user_id: "shiv@athina.ai",
    // your session or conversation ID (OPTIONAL)
    session_id: "c45g-1234-s6g4-43d3",
    // the user's query (for chat applications) (OPTIONAL)
    user_query: user_query,
    // "production" or "development" (OPTIONAL); default value: production
    environment: "production",
    // unique external identifier; should be unique across all inference calls
	external_reference_id: "5e838eaf-7dd0-4b6f-a32c-26110dd54e58",
}
```

## Not using Python?

Reach out to us at hello@athina.ai - we're happy to add support for other stacks as well if we hear from you.
